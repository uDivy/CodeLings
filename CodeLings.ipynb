{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNa3AIG3WDT150KYBz09Zgm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uDivy/CodeLings/blob/development/CodeLings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "Hq844L-NnnBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ztcBzF4OdUpG",
        "outputId": "fb05d79c-c0b6-43d8-e400-e5eecdb5cbec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/dill/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "metadata": {
        "id": "fdk1qiy5nv6g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up project\n",
        "\n"
      ],
      "metadata": {
        "id": "kUZ2ZxjQmeas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up the pre-trained model and tokenizer\n",
        "checkpoint = \"Salesforce/codet5p-220m-bimodal\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(device)"
      ],
      "metadata": {
        "id": "E-UaLVZcgrt6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Download nl2bash-custom dataset\n",
        "dataset = load_dataset(\"AnishJoshi/nl2bash-custom\")\n",
        "\n",
        "# Step 3: Explore the dataset\n",
        "# Convert the dataset to pandas DataFrame for exploration\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "df_valid = pd.DataFrame(dataset['validation'])\n",
        "df_test = pd.DataFrame(dataset['test'])\n",
        "\n",
        "# Print dataset structure\n",
        "print(\"Dataset structure:\")\n",
        "print(df_train.info())\n",
        "\n",
        "# Print size of each split\n",
        "print(f\"Train size: {len(df_train)}\")\n",
        "print(f\"Validation size: {len(df_valid)}\")\n",
        "print(f\"Test size: {len(df_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiL3LCU_oR5H",
        "outputId": "88f3fe60-8f42-432d-dfeb-4b5c3f9c38ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset structure:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19658 entries, 0 to 19657\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   bash_code   19658 non-null  object\n",
            " 1   nl_command  19658 non-null  object\n",
            " 2   srno        19658 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 460.9+ KB\n",
            "None\n",
            "Train size: 19658\n",
            "Validation size: 2457\n",
            "Test size: 2458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define Mean Reciprocal Rank (MRR) metric\n",
        "def mrr_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Mean Reciprocal Rank (MRR) given the true and predicted labels.\n",
        "    Args:\n",
        "        y_true (list of str): List of true target values (bash commands).\n",
        "        y_pred (list of str): List of predicted target values (bash commands).\n",
        "    Returns:\n",
        "        float: MRR score.\n",
        "    \"\"\"\n",
        "    mrr_total = 0.0\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        # Convert true and pred to tokenized form\n",
        "        true_tokenized = tokenizer(true, return_tensors=\"pt\").input_ids.to(device)\n",
        "        pred_tokenized = tokenizer(pred, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "        # Find the minimum length between the two sequences\n",
        "        min_len = min(true_tokenized.size(1), pred_tokenized.size(1))\n",
        "\n",
        "        # Compare token sequences up to the minimum length\n",
        "        rank = (true_tokenized[:, :min_len] == pred_tokenized[:, :min_len]).nonzero(as_tuple=True)[1] + 1\n",
        "\n",
        "        if len(rank) > 0:\n",
        "            mrr_total += 1 / rank[0].item()\n",
        "\n",
        "    return mrr_total / len(y_true)\n",
        "\n",
        "# Step 5: Evaluate the pre-trained model on the dataset using MRR\n",
        "def evaluate_model(dataset, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the dataset using Mean Reciprocal Rank (MRR).\n",
        "    Args:\n",
        "        dataset: Dataset to evaluate the model on.\n",
        "        model: Pre-trained model.\n",
        "        tokenizer: Pre-trained tokenizer.\n",
        "    Returns:\n",
        "        MRR score.\n",
        "    \"\"\"\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Iterate through the first 50 samples in the dataset\n",
        "    for index, row in tqdm(dataset.iterrows(), total=len(dataset)):\n",
        "        input_text = row['nl_command']\n",
        "        true_output = row['bash_code']\n",
        "\n",
        "        # Tokenize the input and generate predictions\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "        outputs = model.generate(input_ids)\n",
        "        predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        y_true.append(true_output)\n",
        "        y_pred.append(predicted_output)\n",
        "\n",
        "        if len(y_true) >= 50:  # Restrict to the first 50 test cases\n",
        "            break\n",
        "\n",
        "    # Calculate MRR\n",
        "    mrr = mrr_score(y_true, y_pred)\n",
        "    return mrr\n",
        "\n",
        "# Step 6: Evaluate on the test set (limit to the first 50 rows)\n",
        "test_dataset = df_test.head(50)  # Restrict to first 50 samples\n",
        "\n",
        "mrr_result = evaluate_model(test_dataset, model, tokenizer)\n",
        "print(f\"\\nMRR on the first 50 test cases: {mrr_result:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3khOlkW1g53P",
        "outputId": "64c10dd2-1fbb-44fb-f0d9-582e38184f1d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 49/50 [01:21<00:01,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRR on the first 50 test cases: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to randomly pick an nl_command, generate the bash_code from the model, and compare with the original\n",
        "def random_nl_command_evaluation(test_dataset, model, tokenizer):\n",
        "    # Choose a random index from the test dataset\n",
        "    random_index = 1600 # random.randint(0, len(test_dataset) - 1)\n",
        "\n",
        "    # Extract the nl_command and the corresponding original bash_code\n",
        "    random_sample = test_dataset.iloc[random_index]\n",
        "    input_text = random_sample['nl_command']\n",
        "    original_bash_code = random_sample['bash_code']\n",
        "\n",
        "    # Tokenize the input nl_command and generate the bash_code using the model\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "    outputs = model.generate(input_ids)\n",
        "    generated_bash_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Print the original and generated bash_code\n",
        "    print(f\"Random NL Command: {input_text}\")\n",
        "    print(f\"Original Bash Code: {original_bash_code}\")\n",
        "    print(f\"Generated Bash Code: {generated_bash_code}\")\n",
        "\n",
        "# Call the function with the test dataset\n",
        "random_nl_command_evaluation(df_test, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-YjlbYvl8vo",
        "outputId": "0bba9eb1-9ad8-4804-f50d-b1e0b66fa19d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random NL Command: Search all regular files in the current directory for \"example\"\n",
            "Original Bash Code: find -maxdepth 1 -type f | xargs grep -F 'example'\n",
            "Generated Bash Code: def search_example ( self, example_path ) : for path in os. listdir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1K2eaa0qmB7T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}